Sqoop

--List-databases

sqoop list-databases \
--connect="jdbc:mysql://nn01.itversity.com:3306/" \
--username=retail_dba \
--password=itversity

--List tables
sqoop list-tables \
  --connect "jdbc:mysql://nn01.itversity.com:3306/retail_db" \
  --username retail_dba \
  --password itversity \


--Eval Command to fire a query on MYSQL directly using sqoop
sqoop eval \
  --connect "jdbc:mysql://nn01.itversity.com:3306/retail_db" \
  --username retail_dba \
  --password itversity \
  --query "select count(1) from order_items"
  
--To import all tables from MYSQL to HDFS  
--First Make a directory on HDFS where tables will be imported.

hadoop fs -mkdir /user/rajsharmaplus/sqoop_import

--Next import all tables in database retail_db  
  
sqoop import-all-tables \
-m 1 \
--connect  "jdbc:mysql://nn01.itversity.com:3306/retail_db" \
--username=retail_dba \
--password=itversity \
--warehouse-dir=/user/rajsharmaplus/sqoop_import

--Import all tables using sqoop-import into existing hive database (retail_db)()MYSQL to HIVE)


sqoop import-all-tables \
--m 1 \
--connect "jdbc:mysql://nn01.itversity.com:3306/retail_db" \
--username=retail_dba \
--password=itversity \
--hive-import \
--hive-overwrite \
--create-hive-table \
--outdir java_files \
--hive-database sqoop_import_lab

--Validate import of tables from MYSQL to HIVE using below command:

hive -e "use sqoop_import_lab; SHOW TABLES; SELECT * FROM departments;"

--Import single table departments using sqoop-import into HDFS location /user/cloudera/departments

--Avro files-http://blog.cloudera.com/blog/2012/12/how-to-use-a-serde-in-apache-hive/
--Sequence Files-SequenceFile is a flat file consisting of binary key/value pairs. It is extensively used in MapReduce as input/output formats.
It is also worth noting that, internally, the temporary outputs of maps are stored using SequenceFile.


--Parquet Files:Apache Parquet is a columnar storage format available to any project in the Hadoop ecosystem, 
regardless of the choice of data processing framework, data model or programming language.


-----Export to MYSQL from HDFS and HIVE------------



mysql -u retail_dba -h nn01.itversity.com -p itversity;
CREATE TABLE departments_export_lab AS SELECT * FROM retail_db.departments WHERE 1=2;


--------------------------------------------
--The following command connects to mysql and loads data to mysql(exports to mysql) from HDFS
  
	   
sqoop export --connect "jdbc:mysql://nn01.itversity.com:3306/retail_db" \
--username retail_dba \
--password itversity \
--table departments_export_lab  \
--export-dir /user/rajsharmaplus/sqoop_import/departments \
--input-fields-terminated-by ',' \
--input-lines-terminated-by '\n' \
--num-mappers 2 \
--outdir java_files	   \




--Change the delimiter and file format of data during import using Sqoop
 

CREATE DATABASE IF NOT EXISTS retail_stage_lab;
USE retail_stage;
CREATE TABLE departments (
department_id INT,
department_name STRING
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'
STORED AS TEXTFILE;
SHOW TABLES;

---------

--Change field delimter to "|"  while importing table departments to HDFS
sqoop import \
--connect "jdbc:mysql://nn01.itversity.com:3306/retail_db" \
--username=retail_dba \
--password=itversity \
--table departments \
--fields-terminated-by '|' \
--lines-terminated-by '\n' \
--hive-home /apps/hive/warehouse/ \
--hive-import \
--hive-overwrite \
--hive-table departments \
--hive-database retail_stage_lab \
--outdir java_files \


