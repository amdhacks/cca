
##conf "spark.ui.port=10111" 
##pyspark --conf "spark.ui.port=10105"
##Read a text file and check its contents
from pyspark import SparkContext

dataRDD = sc.textFile("/user/rajsharmaplus/spark/testfile.txt")
for line in dataRDD.collect():
    print(line)

print(dataRDD.count())

dataRDD.saveAsTextFile("/user/cloudera/pyspark/departments")

dataRDD.map(lambda x: (None, x)).saveAsSequenceFile("/user/rajsharmaplus/sqoop_import/departmentsSeq1")
#dataRDD.map(lambda x: tuple(x.split(",", 1))).saveAsSequenceFile("/user/rajsharmaplus/sqoop_import/departmentsSeq2")
#dataRDD.map(lambda x: tuple(x.split(",", 1))).saveAsSequenceFile("/user/rajsharmaplus/sqoop_import/orders1")


#reading sequence file
data = sc.sequenceFile("/user/rajsharmaplus/sqoop_import/departmentsSeq1")
for rec in data.collect():print(rec)



#Code snippet to read data from sequence files with key
data = sc.sequenceFile("/user/rajsharmaplus/sqoop_import/departmentsSeq1", "org.apache.hadoop.io.IntWritable", "org.apache.hadoop.io.Text")
for rec in data.collect():print(rec)

#Code snippet to read data from hive tables in hive context. In 1.2.x, it might not run.
from pyspark.sql import HiveContext
sqlContext = HiveContext(sc)
depts = sqlContext.sql("select * from departments")
for rec in depts.collect(): print(rec)

##Word Count Program
#Reading data from HDFS location
from pyspark import SparkContext, SparkConf
conf = SparkConf().setAppName("pyspark")
sc = SparkContext(conf=conf)

data = sc.textFile("/user/rajsharmaplus/spark/wordcount_ip")
#Flatten each line into multiple words using " " (space) as delimiter.
dataFlatMap = data.flatMap(lambda x: x.split(" "))
#Associate value 1 for each of the input word to map function.
dataMap = dataFlatMap.map(lambda x: (x, 1))
#Aggregating using key (which are nothing but all unique words)
dataReduceByKey = dataMap.reduceByKey(lambda x,y: x + y)
#Saving to HDFS
dataReduceByKey.saveAsTextFile("/user/rajsharmaplus/spark/sk_wordcount_out1")
#Validating the data set
for i in dataReduceByKey.collect():print(i)

#Extract the key from orders and order_items (using map)
#Join the orders and order_items
#Get revenue per order item per day
#Get order count per date from order_items (aggregation). As there are orders which do not have corresponding records in order_items, we cannot get count using order table. We need to join order_items with orders to get total number of orders per day.
#Get revenue per day from joined data

#Reading the data from both orders and order_items
from pyspark import SparkContext, SparkConf
conf = SparkConf().setAppName("pyspark")
sc = SparkContext(conf=conf)
ordersRDD = sc.textFile("/user/rajsharmaplus/sqoop_import/orders")
orderItemsRDD = sc.textFile("/user/rajsharmaplus/sqoop_import/order_items")

#Read the data from orders and order_items
ordersParsedRDD = ordersRDD.map(lambda rec: (int(rec.split(",")[0]), rec))
orderItemsParsedRDD = orderItemsRDD.map(lambda rec: (int(rec.split(",")[1]), rec))

#Join data sets using spark transformation join
ordersJoinOrderItems = orderItemsParsedRDD.join(ordersParsedRDD)
revenuePerOrderPerDay = ordersJoinOrderItems.map(lambda t: (t[1][1].split(",")[1], float(t[1][0].split(",")[4])))

ordersPerDay = ordersJoinOrderItems.map(lambda rec: rec[1][1].split(",")[1] + "," + str(rec[0])).distinct()
#Problem statement: get the revenue and number of orders from order_items on daily basis. Here are the steps.
ordersPerDayParsedRDD = ordersPerDay.map(lambda rec: (rec.split(",")[0], 1))
totalOrdersPerDay = ordersPerDayParsedRDD.reduceByKey(lambda x, y: x + y)

totalRevenuePerDay = revenuePerOrderPerDay.reduceByKey( 
lambda total1, total2: total1 + total2 
)


for data in totalRevenuePerDay.collect():
  print(data)
  
finalJoinRDD = totalOrdersPerDay.join(totalRevenuePerDay)
for data in finalJoinRDD.take(5):
  print(data)  
  



	
